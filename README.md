# Steps
First use list_images.py to list 100 images

Then use extract_feats_batch.py to get visual features  which internally call extract_fasterrcnn_features which uses torchvision.models.detection.fasterrcnn_resnet50_fpn from images -> "images_list.txt" (generated by list_images.py)

Then verify_feature_forward.py to check if the features are working fine using command line features too (OPTIONAL)


Then we use filter_annotations_by_sampled_images.py to cut down the used .csv file in final training. 

Finally use HF_HOME=/media/user/VQA_Raghuveer_Priyanshu/hf_cache python3 train_fusion_vqa.py


       ┌───────────────┐                     ┌───────────────┐
Input  │   Question    │                     │   Question    │
Text   └───────┬───────┘                     └───────┬───────┘
               │                                     │
               ▼                                     ▼
        ┌───────────────┐                     ┌───────────────┐
Input   │     ViLT      │                     │    LXMERT     │
Image → │ - Text        │   Input Image →    │ - Text        │
        │ - Image       │    Faster-RCNN     │ - Visual Feats│
        │ (PIL + Trans.)│    Features        │ - Boxes       │
        └───────┬───────┘                     └───────┬───────┘
                │                                     │
                │ ViLT pooled embedding               │ LXMERT pooled embedding
                │   (768-dim)                         │   (768-dim)
                └──────────────┬─────────────────────┘
                               ▼
                    ┌─────────────────────┐
                    │  Late Fusion /      │
                    │  Concatenation      │
                    │   (768 + 768 = 1536)│
                    └─────────┬──────────┘
                              ▼
                       ┌─────────────┐
Output                  │ Fully Conn. │
Answer                  │ + ReLU +    │
(answer_idx)            │ Dropout     │
                        └─────┬──────┘
                              ▼
                        ┌─────────────┐
                        │ Final Answer│
                        └─────────────┘






ALSO 



## Pros of using LXMERT with only text (no visual features)

- Simpler setup
- No need to download or generate .pt feature files.
- No need to worry about alignment of images with pre-extracted regions.
- Easier to debug and faster to iterate.
- Faster training and lower memory
- LXMERT won’t process visual embeddings → fewer tensors in GPU memory.
- Training loop is lighter, especially useful for small-scale experiments like your 100-sample run.
- Avoids missing/incorrect features
- You don’t risk silent errors like all-zero visual features (which could mislead the model).
- Consistency is easier because text alone is deterministic.
- Focus on language reasoning
- If your questions are strongly language-based (“what color is the sky?”, “what is the man doing?”), a text-only LXMERT can still capture some biases and correlations.

## Cons of using LXMERT with only text

- No visual grounding
- The model can’t “see” the image. 
- Hard to answer questions that require understanding spatial relationships or object attributes.
- Example: “Is the man wearing a helmet?” → impossible to answer correctly without visual features.
- Lower overall accuracy
- LXMERT’s pretrained weights are multimodal, and ignoring visual features underutilizes its knowledge.
- Many VQA datasets have biases in language, but vision + language is key for robust performance.
- Limited multimodal fusion benefits
- Your ViLT + LXMERT fusion now becomes mostly a ViLT visual+text model + text-only LXMERT.
- LXMERT isn’t contributing multimodal information, so late fusion might not add much.
- Model may learn to ignore visual cues
- During training, the network might overfit to language patterns.
- Could fail when test data requires actual image comprehension.




         ┌───────────────┐
Input    │   Question    │
Text     └───────┬───────┘
                 │
                 ▼
         ┌───────────────┐
         │    LXMERT     │
         │  - Text Only  │
         └───────┬───────┘
                 │
                 │ LXMERT pooled embedding
                 │   (768-dim)
                 ────────────────┐
                                 │
         ┌───────────────┐       │
Input    │   Question    │       │
Text     └───────┬───────┘       │
                 │               │
                 ▼               │
         ┌───────────────┐       │
Input    │     ViLT      │       │
Image →  │  - Text       │       │
         │  - Image      │       │
         └───────┬───────┘       │
                 │
                 │ ViLT pooled embedding
                 │   (768-dim)
                 ────────────────┘
                          │
                          ▼
          ┌────────────────────────┐
          │  Concatenation (Late   │
          │       Fusion)          │
          └───────────┬────────────┘
                      │
                      ▼
             ┌─────────────┐
             │  Final Answer│
             └─────────────┘
